{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d786ea",
   "metadata": {},
   "source": [
    "# Problem 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f4b9f",
   "metadata": {},
   "source": [
    "# Problem 1. Convexity and least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a467b6",
   "metadata": {},
   "source": [
    "## 1.1\n",
    "\n",
    "Note that\n",
    "$$f(x) = \\|b - Ax\\|^2 = (b - Ax)^T(b - Ax) = \\underbrace{b^Tb - 2b^TAx}_{f_1(x)} + \\underbrace{x^TA^TAx}_{f_2(x)}$$\n",
    "and for any $x, y, \\alpha\\in(0, 1)$, \n",
    "$$f_1(x + \\alpha(y - x)) = b^Tb - 2b^TA(x + \\alpha(y - x)) = \\alpha(b^Tb - 2b^TAy) + (1 - \\alpha)(b^Tb - 2b^TAx) = \\alpha f_1(y) + (1 - \\alpha)f_1(x)$$.\n",
    "\n",
    "Also, Let $Q = A^TA$, we can see $Q^T = Q$ and thus $Q$ is symmetric. Moreover, for any vector x, $x^TQx = x^TA^TAx = \\|Ax\\|^2\\ge 0$, $Q$ is thus positive semi-definite. By the conclusion from the last homework, we know $f_2(x)$ is convex. \n",
    "\n",
    "Consequently, \n",
    "$$f(x + \\alpha(y - x)) = f_1(x + \\alpha(y - x)) + f_2(x + \\alpha(y - x))\\le \\alpha f_1(y) + (1 - \\alpha)f_1(x) + \\alpha f_2(y) + (1 - \\alpha)f_2(x) = \\alpha f(x) + (1 - \\alpha)f(x).$$\n",
    "Hence, $f(x)$ is convex. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8477d66",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "\n",
    "Given any matrix $A$, the corresponding null space is \n",
    "$$\\text{Null}(A) = \\{x|Ax = 0\\}$$\n",
    "Take $x, y\\in\\text{Null}(A)$, it satisfies that $Ax = Ay = 0$. Thus, for any $\\alpha\\in(0, 1)$, we have \n",
    "$$A(x + \\alpha(y - x)) = \\alpha Ay + (1 - \\alpha)Ax = 0$$\n",
    "which means $x + \\alpha(y - x)\\in\\text{Null}(A)$ and thus $\\text{Null}(A)$ is convex set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47cd69",
   "metadata": {},
   "source": [
    "# Problem 2. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc22c7",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "\n",
    "Let $f_\\lambda(x) = \\|b - Ax\\|_2^2 + \\lambda\\|x\\|_2^2$. A necessary condition for a minimizer is $f_\\lambda^\\prime(x) = 0$, i.e., \n",
    "$$-2A^T(b - Ax) +2\\lambda x = 0$$\n",
    "which is equivalent to \n",
    "$$(A^TA + \\lambda I)x = A^Tb\\tag{1}$$\n",
    "\n",
    "For any vector $y$, we have \n",
    "$$y^T(A^TA + \\lambda I)y = \\|Ay\\|^2 + \\lambda\\|y\\|^2 > 0$$ \n",
    "and thus $A^TA+ \\lambda I$ is positive definite and thus has full rank. Therefore, equation (1) has unique solution\n",
    "$$x^* = (A^TA + \\lambda I)^{-1}A^Tb$$\n",
    "\n",
    "We will prove $x^*$ is the global minimizer by verifying $f_\\lambda(x)$ is (strictly) convex. From Problem 1, we know that $\\|b - Ax\\|_2^2$ is convex. Also, \n",
    "$$\\lambda\\|x\\|_2^2 = x^T(\\lambda I)x$$\n",
    "and $\\lambda I$ is positive definite, we know $\\lambda\\|x\\|_2^2$ is (strictly) convex. Consequently, $f_\\lambda(x)$ is (strictly) convex (one can use the same argument as in 1.1). Hence, $x^*$ is the unique solution to ridge regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d709d",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "\n",
    "Suppose $A$ is a $p\\times q$ matrix, we can conduct SVD on A as follows \n",
    "$$A = U\\Sigma V^T$$\n",
    "where $U$ is  a $p\\times q$ unitary matrix, $\\Sigma$ is a $q\\times q$ diagonal matrix and $V$ is a $q\\times q$ unitary matrix. We have \n",
    "$$x^* = (A^TA + \\lambda I)^{-1}A^Tb = (V\\Sigma U^TU\\Sigma V^T + \\lambda I)^{-1}V\\Sigma U^Tb$$\n",
    "$$=(V\\Sigma^2V^T + \\lambda I)^{-1}V\\Sigma U^Tb = [V(\\Sigma^2 + \\lambda I)V^T]^{-1}V\\Sigma U^Tb$$\n",
    "$$=V(\\Sigma^2 + \\lambda I)^{-1}V^TV\\Sigma U^Tb = V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma U^Tb\\tag{2}$$\n",
    "Now, suppose $V = (v_1, \\dots, v_q), U = (u_1, \\dots, u_q), \\Sigma = \\text{diag}\\{s_1, \\dots, s_q\\}$ and $b = (b_1, \\dots, b_p)^T$, we can express equation (2) as \n",
    "$$x^* = \\sum_{i = 1}^q\\frac{s_i}{s_i^2 + \\lambda}v_iu_i^Tb_i\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857210a6",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "\n",
    "Let $\\lambda\\to\\infty$, we get $x^*\\to 0$. \n",
    "\n",
    "Let $\\lambda\\to 0$, we get $x^*\\to\\sum_{i = 1}^qs_i^{-1}v_iu_i^Tb_i$, which can be easily seen to be ols solution since \n",
    "$$x_\\text{ols} = (A^TA)^{-1}A^Tb = (V\\Sigma^2V^T)^{-1}V\\Sigma U^Tb = V\\Sigma^{-1}U^Tb= \\sum_{i = 1}^qs_i^{-1}v_iu_i^Tb.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48c507",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "\n",
    "The code is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52df7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Plots, Random, SparseArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e06567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "teams = [\"duke\",\"miami\",\"unc\",\"uva\",\"vt\"]\n",
    "data = [ # team 1 team 2, team 1 pts, team 2 pts\n",
    "    1 2  7 52 # duke played Miami and lost 7 to 52 \n",
    "    1 3 21 24 # duke played unc and lost 21 to 24 \n",
    "    1 4  7 38\n",
    "    1 5  0 45\n",
    "    2 3 34 16\n",
    "    2 4 25 17\n",
    "    2 5 27  7\n",
    "    3 4  7  5\n",
    "    3 5  3 30\n",
    "    4 5 14 52\n",
    "]\n",
    "ngames = size(data,1)\n",
    "nteams = length(teams)\n",
    "\n",
    "G = zeros(ngames, nteams)\n",
    "p = zeros(ngames, 1)\n",
    "\n",
    "for g=1:ngames\n",
    "    i = data[g,1]\n",
    "    j = data[g,2]\n",
    "    Pi = data[g,3]\n",
    "    Pj = data[g,4]\n",
    "  \n",
    "    G[g,i] = 1\n",
    "    G[g,j] = -1\n",
    "    p[g] = Pi - Pj\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1e9751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5Ã—1 Matrix{Float64}:\n",
       " -124.00000000000001\n",
       "   91.00000000000003\n",
       "  -40.00000000000001\n",
       "  -17.000000000000014\n",
       "   90.00000000000001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For lambda = 0\n",
    "\n",
    "F = svd(G)\n",
    "ridge_zero = transpose(F.Vt) * Diagonal(F.S) * transpose(F.U) * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0739bd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ridge_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae68837",
   "metadata": {},
   "source": [
    "As we can see, the rank of the five teams is: Duke < UNC < UVA < VT < Miami, which corresponds to what we got in class. However, recall that in class we solved a constrained least square problem by constraining the sum of the scores to be a constant, here instead, without any constraint, we get essentially a zero-sum result. \n",
    "\n",
    "For $\\lambda = \\infty$, the score vector would be $0$, which is noninformative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280758f",
   "metadata": {},
   "source": [
    "## 2.5\n",
    "\n",
    "First consider the partition of $A$ and $x$: \n",
    "$$A = \\begin{pmatrix} A_1 & \\tilde{A}\\end{pmatrix}$$\n",
    "$$x = \\begin{pmatrix} x_1 & \\tilde{x}^T\\end{pmatrix}^T$$\n",
    "where $A_1$ has one column and $x_1$ is a scalar.\n",
    "\n",
    "The objective function is then \n",
    "$$f_\\lambda(x) = \\|b - A_1x_1 - \\tilde{A}\\tilde{x}\\|_2^2 + \\lambda x_1^2 = \\|b - \\tilde{A}\\tilde{x}\\|_2^2 + \\|A_1x_1\\|_2^2 - 2x_1A_1^T(b - \\tilde{A}\\tilde{x}) + \\lambda x_1^2$$\n",
    "Now, we take partial derivative w.r.t. $\\tilde{x}$ and $x_1$, respectively, \n",
    "$$\\frac{\\partial f_\\lambda(x)}{\\partial\\tilde{x}} = -2\\tilde{A}^T(b - \\tilde{A}\\tilde{x}) + 2\\tilde{A}^TA_1x_1\\tag{4}$$\n",
    "$$\\frac{\\partial f_\\lambda(x)}{\\partial x_1} = 2A_1^TA_1x_1 - 2A_1^T(b - \\tilde{A}\\tilde{x}) + 2\\lambda x_1\\tag{5}$$\n",
    "By (5) equal to zero, we get \n",
    "$$x_1 = (A_1^TA_1 + \\lambda)^{-1}(A_1^Tb - A_1^T\\tilde{A}\\tilde{x}):= a(A_1^Tb - A_1^T\\tilde{A}\\tilde{x})$$\n",
    "which can be substituted to (4) and yields \n",
    "$$\\tilde{A}^T(I - aA_1A_1^T)\\tilde{A}\\tilde{x} = \\tilde{A}^T(I - aA_1A_1^T)b$$\n",
    "\n",
    "We can solve the former equation to get $x_1, \\tilde{x}_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a132f",
   "metadata": {},
   "source": [
    "# Problem 3. Thinking about constraints\n",
    "\n",
    "Substituting $y = 5x + 2$ into the function, we get \n",
    "$$f(x, y) = x^2 + 2(5x + 2)^2 = 51x^2 + 40x + 8 = 51\\left(x + \\frac{20}{51}\\right)^2 + \\frac{8}{51}\\ge\\frac{8}{51}$$\n",
    "The function is minimized at $\\left(-\\frac{20}{51}, \\frac{2}{51}\\right)$, with minimum value $8 / 51$. \n",
    "This is a global minimizer because any other point along the line will produce larger value.\n",
    "\n",
    "The gradient at $\\left(-\\frac{20}{51}, \\frac{2}{51}\\right)$ is \n",
    "$$\\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right)\\bigg|_{(x, y) = \\left(-\\frac{20}{51}, \\frac{2}{51}\\right)} = (2x, 4y)\\bigg|_{(x, y) = \\left(-\\frac{20}{51}, \\frac{2}{51}\\right)} = \\left(-\\frac{40}{51}, \\frac{8}{51}\\right)$$. \n",
    "\n",
    "Without the constraint, the minimizer would be $(0, 0)$ with gradient also $(0, 0)$. So the difference lies in the gradient: under constraint, the gradient of the minimizer may not be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc60bf",
   "metadata": {},
   "source": [
    "# Problem 4. Alternate formulations of Least Squares\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76ab73",
   "metadata": {},
   "source": [
    "## 4.1 & 4.2 & 4.3\n",
    "\n",
    "We can rewrite the optimization problem as \n",
    "$$\\min_{y}\\frac{1}{2}\\|b - Cy\\|_2^2\\quad\\text{s.t. }Cy = b - r$$\n",
    "The Lagragian is \n",
    "$$\\mathcal{L}(y; \\lambda) = \\frac{1}{2}\\|b - Cy\\|_2^2 - \\lambda^T(Cy - b + r)$$\n",
    "And we get \n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial y} = -C^T(b - Cy) - C^T\\lambda = 0$$\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial\\lambda} = Cy + r - b = 0$$\n",
    "and the augmented system is \n",
    "$$\\begin{pmatrix}C^TC & C^T\\\\C & 0\\end{pmatrix}\\begin{pmatrix}y\\\\-\\lambda\\end{pmatrix} = \\begin{pmatrix}C^Tb\\\\ b - r\\end{pmatrix}\\tag{6}$$\n",
    "Since rank$(C^TC) = \\text{rank}(C) = n$, we know $C^TC$ is invertible. Let \n",
    "$$A = \\begin{pmatrix}C^TC & C^T & C^Tb\\\\C & 0 & b - r\\end{pmatrix}$$\n",
    "be the augmented matrix of the linear system.\n",
    "\n",
    "By left multiplying $-C(C^TC)^{-1}$ on the first row of $A$ and add it to the second row, we get another matrix \n",
    "$$A_1 = \\begin{pmatrix} C^TC & C^T & C^Tb\\\\0 & -C(C^TC)^{-1}C^T & b - r - C(C^TC)^{-1}C^Tb\\end{pmatrix}$$\n",
    "Then, we left multiply $C^T$ on the second row of $A_1$ and add it to the first row to get a diagonal partition matrix\n",
    "$$A_2 = \\begin{pmatrix}C^TC & 0 & C^Tb + C^Tb - C^Tr - C^Tb\\\\0 & C(C^TC)^{-1}C^T & b - r - C(C^TC)^{-1}C^Tb\\end{pmatrix}$$\n",
    "Now by the theory of linear equation, we know that equation (6) has the same solution as \n",
    "$$\\begin{pmatrix} C^TC & 0\\\\ 0 & C(C^TC)^{-1}C^T\\end{pmatrix}\\begin{pmatrix}y\\\\-\\lambda\\end{pmatrix} = \\begin{pmatrix}C^Tb + C^Tb - C^Tr - C^Tb\\\\b - r - C(C^TC)^{-1}C^Tb\\end{pmatrix}$$\n",
    "We get $\\lambda$ satisfies $-C(C^TC)^{-1}C^T\\lambda = b - r - C(C^TC)^{-1}C^Tb$. Left multiply $C^T$ on both sides, we have \n",
    "$$-C^T\\lambda = C^Tb - C^Tr - C^Tb = -C^Tr = -C^T(b - Cy) = C^TCy - C^Tb$$\n",
    "Since by equation (6), $C^TCy - C^T\\lambda = C^Tb$, we get \n",
    "$$C^TCy + C^TCy - C^Tb = C^Tb\\Rightarrow C^TCy = C^Tb$$\n",
    "\n",
    "Honestly, I don't see any advantage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e87f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
